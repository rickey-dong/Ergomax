{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pose_quality_assessment_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9DMcGOVShzC8/XHundJh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickey-dong/Ergomax/blob/main/pose_quality_assessment_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9rqqJEMjCtR",
        "outputId": "1e7e4e8b-ad56-4848-f400-7b4b70ebc60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "DcKJ45X-jZMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "movenet = model.signatures['serving_default']"
      ],
      "metadata": {
        "id": "37MLHXWojfDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOSE = 0\n",
        "LEFT_EYE = 1\n",
        "RIGHT_EYE = 2\n",
        "LEFT_EAR = 3\n",
        "RIGHT_EAR = 4\n",
        "LEFT_SHOULDER = 5\n",
        "RIGHT_SHOULDER = 6\n",
        "Y = 0\n",
        "X = 1\n",
        "CONFIDENCE_VALUE = 2\n",
        "CONFIDENCE_THRESHOLD = 0.25"
      ],
      "metadata": {
        "id": "eAuH4dX-njYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(image_file):\n",
        "    \"\"\"\n",
        "    takes in an image\n",
        "    and\n",
        "    returns a list of relevant landmarks\n",
        "    with y, x, and confidence values\n",
        "    \"\"\"\n",
        "\n",
        "    # run the model!\n",
        "    outputs = movenet(image_file)\n",
        "\n",
        "    keypoints = outputs['output_0'].numpy()\n",
        "\n",
        "    # converting this 4-layered array into something more manageable\n",
        "    # i don't think we're too comfortable working with numpy so maybe\n",
        "    # better to convert\n",
        "    cleaner_2d_array = [ [0]*3 for i in range(7) ]\n",
        "    body_part = 0\n",
        "    for ary0 in keypoints:\n",
        "        for ary1 in ary0:\n",
        "            while body_part < 7:\n",
        "                index = 0\n",
        "                for num in keypoints[0][0][body_part]:\n",
        "                    cleaner_2d_array[body_part][index] = num\n",
        "                    index += 1\n",
        "                body_part += 1\n",
        "\n",
        "    return cleaner_2d_array"
      ],
      "metadata": {
        "id": "HDwyz474jl26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_rickey = \"\"\"\n",
        "0.51898634 0.5141955 0.60066235 \n",
        "0.46213895 0.5748569 0.8942077 \n",
        "0.46386486 0.45547873 0.8542165 \n",
        "0.5035565 0.6509616 0.5807533 \n",
        "0.51048446 0.37870285 0.7500463 \n",
        "0.78193563 0.7620751 0.5300895 \n",
        "0.7900616 0.30074388 0.65386754\n",
        "\"\"\"\n",
        "\n",
        "# [[0.51898634, 0.5141955, 0.60066235], [0.46213895, 0.5748569, 0.8942077], [0.46386486, 0.45547873, 0.8542165], [0.5035565, 0.6509616, 0.5807533], [0.51048446, 0.37870285, 0.7500463], [0.78193563, 0.7620751, 0.5300895], [0.7900616, 0.30074388, 0.65386754]]\n",
        "\n",
        "slouch_rickey = \"\"\"\n",
        "0.59844774 0.519061 0.40708265 \n",
        "0.5301816 0.59195983 0.8289351 \n",
        "0.5341892 0.45447135 0.4498962 \n",
        "0.57492197 0.67865723 0.6057053 \n",
        "0.5780213 0.3705523 0.5283152 \n",
        "0.7645539 0.8212724 0.2530607 \n",
        "0.7899105 0.28375655 0.25369805\n",
        "\"\"\"\n",
        "\n",
        "# [[0.59844774, 0.519061, 0.40708265], [0.5301816, 0.59195983, 0.8289351], [0.5341892, 0.45447135, 0.4498962], [0.57492197, 0.67865723, 0.6057053], [0.5780213, 0.3705523, 0.5283152], [0.7645539, 0.8212724, 0.2530607], [0.7899105, 0.28375655, 0.25369805]]\n"
      ],
      "metadata": {
        "id": "iqFnR1P5kLYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = tf.io.read_file(\"baseline-rickey.jpg\")\n",
        "baseline = tf.compat.v1.image.decode_jpeg(baseline)\n",
        "baseline = tf.expand_dims(baseline, axis=0)\n",
        "baseline = tf.cast(tf.image.resize_with_pad(baseline, 192, 192), dtype=tf.int32)\n",
        "\n",
        "# baseline_array = feature_extraction(baseline)\n",
        "for row in baseline_array:\n",
        "  string = \"\"\n",
        "  for number in row:\n",
        "    string += str(number)\n",
        "    string += \" \"\n",
        "  print(string)\n",
        "\n",
        "current = tf.io.read_file(\"slouch-rickey.jpg\")\n",
        "current = tf.compat.v1.image.decode_jpeg(current)\n",
        "current = tf.expand_dims(current, axis=0)\n",
        "current = tf.cast(tf.image.resize_with_pad(current, 192, 192), dtype=tf.int32)\n",
        "\n",
        "# current_array = feature_extraction(current)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nus9c4vxjtLr",
        "outputId": "4804b278-a621-40e8-bf5b-7e37a049a0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.51898634 0.5141955 0.60066235 \n",
            "0.46213895 0.5748569 0.8942077 \n",
            "0.46386486 0.45547873 0.8542165 \n",
            "0.5035565 0.6509616 0.5807533 \n",
            "0.51048446 0.37870285 0.7500463 \n",
            "0.78193563 0.7620751 0.5300895 \n",
            "0.7900616 0.30074388 0.65386754 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def has_bad_posture(ideal, current):\n",
        "    \"\"\"\n",
        "    takes in two lists of keypoints and confidence values\n",
        "    returns true if user has bad posture currently,\n",
        "    false otherwise\n",
        "    \"\"\"\n",
        "    # useful articles?\n",
        "    # https://medium.com/roonyx/pose-estimation-and-matching-with-tensorflow-lite-posenet-model-ea2e9249abbd\n",
        "    # https://medium.com/@priyaanka.garg/comparison-of-human-poses-with-posenet-e9ffc36b7427\n",
        "    if current[NOSE][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[LEFT_EYE][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[RIGHT_EYE][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[LEFT_EAR][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[RIGHT_EAR][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[LEFT_SHOULDER][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD or \\\n",
        "        current[RIGHT_SHOULDER][CONFIDENCE_VALUE] < CONFIDENCE_THRESHOLD:\n",
        "            # if the model is less than 26% confident about the location of any\n",
        "            # particular body point, then there's not enough data to make a guess\n",
        "            # or could mean that the user is severely slouching and has most of the\n",
        "            # body off camera\n",
        "            return 'NOT ENOUGH DATA'\n",
        "    cos_sims = calculate_cosine_similarity(ideal, current)\n",
        "    for sim_value in cos_sims:\n",
        "        if sim_value < 0.4:\n",
        "            return 'NOT SIMILAR POSTURE'\n",
        "    # if current body landmarks y-coords are greater than the baseline,\n",
        "    # then that means the landmarks are further down than ideal, so could\n",
        "    # be indicative of slouching\n",
        "    if ((current[LEFT_SHOULDER][Y] >= ideal[LEFT_SHOULDER][Y] + 0.01) or (current[RIGHT_SHOULDER][Y] >= ideal[RIGHT_SHOULDER][Y] + 0.01)):\n",
        "        return 'BAD POSTURE 0'\n",
        "    if ((current[LEFT_EAR][Y] >= ideal[LEFT_EAR][Y] + 0.01) or (current[RIGHT_EAR][Y] >= ideal[RIGHT_EAR][Y] + 0.01)):\n",
        "        return 'BAD POSTURE 1'\n",
        "    return 'NICE'\n"
      ],
      "metadata": {
        "id": "JU7bYNVGnnZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(has_bad_posture(baseline_array, current_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsRCqOUFoL7T",
        "outputId": "c3ff9781-7fe1-4518-f771-6298eeeedc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD POSTURE 1\n"
          ]
        }
      ]
    }
  ]
}